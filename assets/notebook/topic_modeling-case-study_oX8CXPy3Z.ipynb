{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "# CASE STUDY - topic modeling and feature engineering\n\n\n[Feature engineering](https://en.wikipedia.org/wiki/Feature_engineering) is the process of using domain knowledge of your data to create features that can be leveraged by machine learning.  That is not a hard definition, because sometimes it is used in a context where features are transformed for machine learning, but the inclusion of domain knowledge is not implied.  \n\nIt is unfortunately common that for large datasets engineered features are not easy to create.  When there are many features generally only a small number play an important roll when it comes to prediction.  Furthermore,  domain insight is even more difficult to fold into the model when there are hundreds or thousands of features to keep in mind.  However, there is a middle ground---much of the worlds knowledge is locked up in language.  In this case study we will use topic modeling to gather insight from text.  Ideally, the result of these types of experiments would be shared with domain experts to further engineer features that are relevant when it comes to your business opportunity."}, {"metadata": {}, "cell_type": "code", "source": "!pip install pyLDAvis", "execution_count": 2, "outputs": [{"output_type": "stream", "text": "Collecting pyLDAvis\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/3a/af82e070a8a96e13217c8f362f9a73e82d61ac8fff3a2561946a97f96266/pyLDAvis-2.1.2.tar.gz (1.6MB)\n\u001b[K     |################################| 1.6MB 1.1MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: wheel>=0.23.0 in /opt/conda/envs/Python-3.6/lib/python3.6/site-packages (from pyLDAvis) (0.32.3)\nRequirement already satisfied: numpy>=1.9.2 in /opt/conda/envs/Python-3.6/lib/python3.6/site-packages (from pyLDAvis) (1.15.4)\nRequirement already satisfied: scipy>=0.18.0 in /opt/conda/envs/Python-3.6/lib/python3.6/site-packages (from pyLDAvis) (1.2.0)\nRequirement already satisfied: pandas>=0.17.0 in /opt/conda/envs/Python-3.6/lib/python3.6/site-packages (from pyLDAvis) (0.24.1)\nCollecting joblib>=0.8.4 (from pyLDAvis)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/5c/cf6a2b65a321c4a209efcdf64c2689efae2cb62661f8f6f4bb28547cf1bf/joblib-0.14.1-py2.py3-none-any.whl (294kB)\n\u001b[K     |################################| 296kB 25.7MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: jinja2>=2.7.2 in /opt/conda/envs/Python-3.6/lib/python3.6/site-packages (from pyLDAvis) (2.10)\nRequirement already satisfied: numexpr in /opt/conda/envs/Python-3.6/lib/python3.6/site-packages (from pyLDAvis) (2.6.9)\nRequirement already satisfied: pytest in /opt/conda/envs/Python-3.6/lib/python3.6/site-packages (from pyLDAvis) (4.2.1)\nRequirement already satisfied: future in /opt/conda/envs/Python-3.6/lib/python3.6/site-packages (from pyLDAvis) (0.17.1)\nCollecting funcy (from pyLDAvis)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ce/4b/6ffa76544e46614123de31574ad95758c421aae391a1764921b8a81e1eae/funcy-1.14.tar.gz (548kB)\n\u001b[K     |################################| 552kB 5.6MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: pytz>=2011k in /opt/conda/envs/Python-3.6/lib/python3.6/site-packages (from pandas>=0.17.0->pyLDAvis) (2018.9)\nRequirement already satisfied: python-dateutil>=2.5.0 in /opt/conda/envs/Python-3.6/lib/python3.6/site-packages (from pandas>=0.17.0->pyLDAvis) (2.7.5)\nRequirement already satisfied: MarkupSafe>=0.23 in /opt/conda/envs/Python-3.6/lib/python3.6/site-packages (from jinja2>=2.7.2->pyLDAvis) (1.1.0)\nRequirement already satisfied: py>=1.5.0 in /opt/conda/envs/Python-3.6/lib/python3.6/site-packages (from pytest->pyLDAvis) (1.7.0)\nRequirement already satisfied: six>=1.10.0 in /opt/conda/envs/Python-3.6/lib/python3.6/site-packages (from pytest->pyLDAvis) (1.12.0)\nRequirement already satisfied: setuptools in /opt/conda/envs/Python-3.6/lib/python3.6/site-packages (from pytest->pyLDAvis) (40.8.0)\nRequirement already satisfied: attrs>=17.4.0 in /opt/conda/envs/Python-3.6/lib/python3.6/site-packages (from pytest->pyLDAvis) (18.2.0)\nRequirement already satisfied: atomicwrites>=1.0 in /opt/conda/envs/Python-3.6/lib/python3.6/site-packages (from pytest->pyLDAvis) (1.3.0)\nRequirement already satisfied: pluggy>=0.7 in /opt/conda/envs/Python-3.6/lib/python3.6/site-packages (from pytest->pyLDAvis) (0.8.1)\nRequirement already satisfied: more-itertools>=4.0.0 in /opt/conda/envs/Python-3.6/lib/python3.6/site-packages (from pytest->pyLDAvis) (5.0.0)\nBuilding wheels for collected packages: pyLDAvis, funcy\n  Building wheel for pyLDAvis (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pyLDAvis: filename=pyLDAvis-2.1.2-py2.py3-none-any.whl size=97712 sha256=47f01a63baa93d459f336782053498c9dd96b8d3d195b60f60b3c2994da8b679\n  Stored in directory: /home/wsuser/.cache/pip/wheels/98/71/24/513a99e58bb6b8465bae4d2d5e9dba8f0bef8179e3051ac414\n  Building wheel for funcy (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for funcy: filename=funcy-1.14-py2.py3-none-any.whl size=32042 sha256=55837ea81082031df729a66d7113ed8afa361b6a0153fe12477340ad2fd1cfc0\n  Stored in directory: /home/wsuser/.cache/pip/wheels/20/5a/d8/1d875df03deae6f178dfdf70238cca33f948ef8a6f5209f2eb\nSuccessfully built pyLDAvis funcy\nInstalling collected packages: joblib, funcy, pyLDAvis\nSuccessfully installed funcy-1.14 joblib-0.14.1 pyLDAvis-2.1.2\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "import os\nimport re\nimport sys\nimport numpy as np\nimport pandas as pd\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_files\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.decomposition import LatentDirichletAllocation\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom string import punctuation, printable\nfrom sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n\ntry:\n    import pyLDAvis\n    import pyLDAvis.sklearn\nexcept:\n    raise Exception(\"'pip install pyldavis' before running this notebook\")\n\npyLDAvis.enable_notebook()    \nplt.style.use('seaborn')\n%matplotlib inline\n\n## supress all warnings (not to be used during development)\nimport warnings\nwarnings.filterwarnings(\"ignore\")", "execution_count": 1, "outputs": [{"output_type": "error", "ename": "Exception", "evalue": "'pip install pyldavis' before running this notebook", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)", "\u001b[0;32m<ipython-input-1-bffd43b7d599>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msklearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyLDAvis'", "\nDuring handling of the above exception, another exception occurred:\n", "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)", "\u001b[0;32m<ipython-input-1-bffd43b7d599>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msklearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'pip install pyldavis' before running this notebook\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;31mException\u001b[0m: 'pip install pyldavis' before running this notebook"]}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Synopsis\n\n   >Goal:  AAVAIL has recently enabled comments on the core streaming service.  The data science team knows that   this will be an incredibly important source of data going forward.  It will be used inform customer retention, product quality, product market fit and more.  Comments are going live next week and being the diligent data scientist that you are your plan is to build a topic modeling pipeline that that will consume the comments and create visualizations that can be used to communicate with domain experts.\n  \n## Outline\n\n1. EDA - summary tables, use tSNE to visualize the data\n2. Create a transfomation pipelines for NMF and LDA\n3. Use ldaviz and wordclouds to get insight into the clusters\n\n## Data\n\nHere we see how to load the data.\n\n* [download the movie review data](http://www.nltk.org/nltk_data)\n* For more examples of applications with these data see [NLTK's book chapter that uses these data](https://www.nltk.org/book/ch06.html)"}, {"metadata": {}, "cell_type": "code", "source": "data_dir = os.path.join(\"..\",\"data\")\nmovie_reviews = load_files(os.path.join(data_dir,\"movie_reviews\"), shuffle=True)\nX = movie_reviews.data\ny = movie_reviews.target\ntarget_names = movie_reviews.target_names\nprint(X[0])", "execution_count": 20, "outputs": [{"name": "stdout", "output_type": "stream", "text": "b\"arnold schwarzenegger has been an icon for action enthusiasts , since the late 80's , but lately his films have been very sloppy and the one-liners are getting worse . \\nit's hard seeing arnold as mr . freeze in batman and robin , especially when he says tons of ice jokes , but hey he got 15 million , what's it matter to him ? \\nonce again arnold has signed to do another expensive blockbuster , that can't compare with the likes of the terminator series , true lies and even eraser . \\nin this so called dark thriller , the devil ( gabriel byrne ) has come upon earth , to impregnate a woman ( robin tunney ) which happens every 1000 years , and basically destroy the world , but apparently god has chosen one man , and that one man is jericho cane ( arnold himself ) . \\nwith the help of a trusty sidekick ( kevin pollack ) , they will stop at nothing to let the devil take over the world ! \\nparts of this are actually so absurd , that they would fit right in with dogma . \\nyes , the film is that weak , but it's better than the other blockbuster right now ( sleepy hollow ) , but it makes the world is not enough look like a 4 star film . \\nanyway , this definitely doesn't seem like an arnold movie . \\nit just wasn't the type of film you can see him doing . \\nsure he gave us a few chuckles with his well known one-liners , but he seemed confused as to where his character and the film was going . \\nit's understandable , especially when the ending had to be changed according to some sources . \\naside form that , he still walked through it , much like he has in the past few films . \\ni'm sorry to say this arnold but maybe these are the end of your action days . \\nspeaking of action , where was it in this film ? \\nthere was hardly any explosions or fights . \\nthe devil made a few places explode , but arnold wasn't kicking some devil butt . \\nthe ending was changed to make it more spiritual , which undoubtedly ruined the film . \\ni was at least hoping for a cool ending if nothing else occurred , but once again i was let down . \\ni also don't know why the film took so long and cost so much . \\nthere was really no super affects at all , unless you consider an invisible devil , who was in it for 5 minutes tops , worth the overpriced budget . \\nthe budget should have gone into a better script , where at least audiences could be somewhat entertained instead of facing boredom . \\nit's pitiful to see how scripts like these get bought and made into a movie . \\ndo they even read these things anymore ? \\nit sure doesn't seem like it . \\nthankfully gabriel's performance gave some light to this poor film . \\nwhen he walks down the street searching for robin tunney , you can't help but feel that he looked like a devil . \\nthe guy is creepy looking anyway ! \\nwhen it's all over , you're just glad it's the end of the movie . \\ndon't bother to see this , if you're expecting a solid action flick , because it's neither solid nor does it have action . \\nit's just another movie that we are suckered in to seeing , due to a strategic marketing campaign . \\nsave your money and see the world is not enough for an entertaining experience . \\n\"\n"}]}, {"metadata": {}, "cell_type": "markdown", "source": "### QUESTION 1\n\nThe main focus of this exercise is to enable visualization of topics, but these topics can be used as additional \nfeatures for prediction tasks.  The goal of this case study is to ensure that you are comfortable with natural language processing pipelines and topic modeling tools. \n\nThere are many ways to process tokens (words, dates, emojis etc).  NLTK is often used to pre-process text data before the tokens are vectorized.  Generally, the tokens are modified via [stemming or lemmatization](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html).  The next code block provides a lemmatization function that makes use of the library [spacy](https://spacy.io/).  You will need to install it and download the English language reference material as follows.  Stopwords are words that are very common or otherwise irrelevant we use a default list here, but it is an important part of NLP pipelines that needs to be customized for the subject area. Use the following function to process the corpus (this can take a few minutes)\n\n```bash\n~$ pip install spacy\n~$ python -m spacy download en\n```\n\nIf you prefer to use NLTK then you could use a simple lemmatizer like the WordLemmatizer."}, {"metadata": {}, "cell_type": "code", "source": "import spacy\nSTOPLIST = ENGLISH_STOP_WORDS\nSTOPLIST = set(list(STOPLIST) + [\"foo\"])\n\nif not 'nlp' in locals():\n    print(\"Loading English Module...\")\n    nlp = spacy.load('en')\n\ndef lemmatize_document(doc, stop_words=None):\n    \"\"\"\n    takes a list of strings where each string is a document\n    returns a processed list of strings\n    \"\"\"\n    \n    if not stop_words:\n        stop_words = set([])\n  \n    ## ensure working with string\n    doc = str(doc)\n\n    # First remove punctuation form string\n    if sys.version_info.major == 3:\n        PUNCT_DICT = {ord(punc): None for punc in punctuation}\n        doc = doc.translate(PUNCT_DICT)\n\n    # remove unicode\n    clean_doc = \"\".join([char for char in doc if char in printable])\n            \n    # Run the doc through spaCy\n    doc = nlp(clean_doc)\n\n    # Lemmatize and lower text\n    tokens = [re.sub(\"\\W+\",\"\",token.lemma_.lower()) for token in doc ]\n    tokens = [t for t in tokens if len(t) > 1]\n    \n    return ' '.join(w for w in tokens if w not in stop_words)    \n\n## example usage\ncorpus = ['\"You can fool some of the people all of the time, and all of the people some of the time, but you can not fool all of the people all of the time\". -- Abraham Lincoln']\nprocessed = [lemmatize_document(doc, STOPLIST) for doc in corpus]\nprint(processed[0])\nprocessed = [lemmatize_document(doc, None) for doc in corpus]\nprint(\"\\n\"+processed[0])", "execution_count": 52, "outputs": [{"name": "stdout", "output_type": "stream", "text": "pron fool people time people time pron fool people time abraham lincoln\n\npron can fool some of the people all of the time and all of the people some of the time but pron can not fool all of the people all of the time abraham lincoln\n"}]}, {"metadata": {}, "cell_type": "code", "source": "## Use stemming or lemmatization to process the corpus\n## YOUR CODE HERE\n", "execution_count": 1, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### QUESTION 2\n\nUse the CountVectorizer from sklearn to vectorize the tokens.\n\nAdditional resources:\n\n* [scikit-learn CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n* [scikit-learn working with text](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html)\n\nBecause this is an exercise in visualization set the `max_features` to something like 1000.  In the context of supervised learning it is reasonable to grid-search to optimize this parameter."}, {"metadata": {}, "cell_type": "code", "source": "## YOUR CODE HERE\n\n", "execution_count": 2, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### QUESTION 3\n\nUse model the corpus with LDA.  For example, you could use something like the following.\n\n```python\nn_topics = 10\nlda_model = LatentDirichletAllocation(n_components=n_topics, max_iter=5,\n                                      learning_method='online',\n                                      learning_offset=50.,\n                                      random_state=0)\n\nlda_model.fit(tf)\n```\n\nYou could use a pipeline here to make it easier to iterate on changes.\n\n* [scikit-learn's LDA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html)\n* [scikit-learn's user guide for LDA](https://scikit-learn.org/stable/modules/decomposition.html#latentdirichletallocation)"}, {"metadata": {}, "cell_type": "code", "source": "## YOUR CODE HERE\n", "execution_count": 3, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## QUESTION 4\n\nVisualize the corpus using [pyldavis](https://github.com/bmabey/pyLDAvis).\n\n```python\npyLDAvis.sklearn.prepare(lda_model,tf, tf_vectorizer, R=20)\n```\n\n* [PyLDAViz documentation](https://pyldavis.readthedocs.io/en/latest)\n* [PyLDAViz demos](https://pyldavis.readthedocs.io/en/latest/readme.html#video-demos)"}, {"metadata": {}, "cell_type": "code", "source": "## YOUR CODE HERE\n", "execution_count": 4, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## QUESTION 5\n\nTry different numbers of clusters until there is decent separation in the visualization"}, {"metadata": {}, "cell_type": "code", "source": "## YOUR CODE HERE\n\n", "execution_count": 5, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "The visualization here can help determine a reasonable number of number of clusters and it can serve as a communication tool.  If the goal was to find topics that are associated with customer profiles then you would likely work with folks in marketing to refine the clustering.  There are a couple of parameters than can be used to modify the clustering and visualization.  The discovery of meaningful topics is a form of feature engineering."}, {"metadata": {}, "cell_type": "markdown", "source": "## QUESTION 6\n\nIf you were to use the topics from this model to inform clustering or supervised learning you would first need to be able to extract and represent them as a matrix.  Along the same lines if you were to populate a report with tabular descriptions of the data then you will need to be able to extract topic representations.  Here is a starter function\n\n```python\ndef get_top_words(model, feature_names, n_top_words):\n    top_words = {}\n    for topic_idx, topic in enumerate(model.components_):\n        _top_words = [feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n        top_words[str(topic_idx)] = _top_words\n    return(top_words)\n```\n\nUse the function to print the top k words for each topic"}, {"metadata": {}, "cell_type": "code", "source": "## YOUR CODE HERE\n\n", "execution_count": 6, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## QUESTION (EXTRA CREDIT) 7\n\nIf you used `fit_transform` on your original tokens you should have a `2000 x k` array where `k` is the number of topics you choose.  Create a PCA or tSNE visualization that projects the tf matrix into lower dimensional space then uses colors to indicate which documents belong to a topic (e.g. probability > 0.5)."}, {"metadata": {}, "cell_type": "code", "source": "## YOUR CODE HERE\n\n", "execution_count": 8, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<iframe src=\"https://player.vimeo.com/video/87110435\" width=\"640\" height=\"360\"  frameborder=\"0\" allowfullscreen></iframe>"}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.6", "language": "python"}, "language_info": {"name": "python", "version": "3.6.8", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 2}